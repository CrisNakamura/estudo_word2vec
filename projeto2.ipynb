{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bibliotecas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import spacy as sp\n",
    "import numpy as np\n",
    "import logging\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.models import KeyedVectors\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Carregando os dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dados_treino = pd.read_csv('Dados/treino.csv')\n",
    "dados_teste = pd.read_csv('Dados/teste.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pré-processamento com Spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Carregando o modelo de lingua portuguesa.\n",
    "nlp = sp.load('pt_core_news_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Criando um gerador de textos para tratamento.\n",
    "textos_para_tratamento = (titulos.lower() for titulos in dados_treino['title'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Criando uma função para tratamento de texto.\n",
    "def tratatamento_texto(doc):\n",
    "    tokens_validos = []\n",
    "    for token in doc:\n",
    "        e_valido = not token.is_stop and token.is_alpha # Token é válido se não for stopword e for alfabético\n",
    "        if e_valido: # Se for token válido, adiciona na lista de tokens válidos.\n",
    "            tokens_validos.append(token.text)\n",
    "\n",
    "    if len(tokens_validos) > 2: # Se o texto tiver mais de 2 tokens válidos, retorna o texto tratado.\n",
    "        return ' '.join(tokens_validos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "texto = 'Um texto qualquer para ser tratado.'\n",
    "doc = nlp(texto)\n",
    "\n",
    "# Criando uma lista de textos tratados.\n",
    "textos_tratados = [tratatamento_texto(doc) for doc in nlp.pipe(textos_para_tratamento,\n",
    "                                                                batch_size=1000,\n",
    "                                                                n_process=-1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>titulo</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>polêmica marine le pen abomina negacionistas h...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>macron le pen turno frança revés siglas tradic...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>apesar larga vitória legislativas macron terá ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>governo antecipa balanço alckmin anuncia queda...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>queda maio atividade econômica sobe junho bc</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              titulo\n",
       "0  polêmica marine le pen abomina negacionistas h...\n",
       "1  macron le pen turno frança revés siglas tradic...\n",
       "2  apesar larga vitória legislativas macron terá ...\n",
       "3  governo antecipa balanço alckmin anuncia queda...\n",
       "4       queda maio atividade econômica sobe junho bc"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Criando um dataframe com os textos tratados.\n",
    "titulos_tratados = pd.DataFrame({'titulo': textos_tratados})\n",
    "titulos_tratados.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Criando o modelo Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Criando um modelo Word2Vec\n",
    "w2v_modelo = Word2Vec(sg=0, # CBOW\n",
    "                      window=2, # Considerando 2 palavras antes e depois.\n",
    "                      vector_size=300, # Tamanho do vetor do Word2Vec\n",
    "                      min_count=5, # Considerar apenas palavras que aparecem 5 vezes ou mais. \n",
    "                      alpha=0.03, # Taxa de aprendizado.\n",
    "                      min_alpha=0.007) # Taxa de aprendizado (minima)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Construindo o vocabulário do modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "90000\n",
      "84466\n"
     ]
    }
   ],
   "source": [
    "# Imprimindo o tamanho do vocabulário antes do tratamento.\n",
    "print(len(titulos_tratados))\n",
    "\n",
    "# Removendo linhas com valores nulos e duplicados.\n",
    "titulos_tratados = titulos_tratados.dropna().drop_duplicates()\n",
    "\n",
    "# Imprimindo o tamanho do vocabulário após o tratamento.\n",
    "print(len(titulos_tratados))\n",
    "\n",
    "# Criando uma lista de listas de tokens.\n",
    "lista_lista_tokens = [titulo.split(' ') for titulo in titulos_tratados['titulo']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-01-18 14:53:02,119 : - Word2Vec lifecycle event {'params': 'Word2Vec<vocab=0, vector_size=300, alpha=0.03>', 'datetime': '2024-01-18T14:53:02.119932', 'gensim': '4.3.0', 'python': '3.11.5 | packaged by Anaconda, Inc. | (main, Sep 11 2023, 13:26:23) [MSC v.1916 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.22621-SP0', 'event': 'created'}\n",
      "2024-01-18 14:53:02,120 : - collecting all words and their counts\n",
      "2024-01-18 14:53:02,121 : - PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "2024-01-18 14:53:02,136 : - PROGRESS: at sentence #5000, processed 31930 words, keeping 10193 word types\n",
      "2024-01-18 14:53:02,145 : - PROGRESS: at sentence #10000, processed 63848 words, keeping 14989 word types\n",
      "2024-01-18 14:53:02,153 : - PROGRESS: at sentence #15000, processed 95753 words, keeping 18279 word types\n",
      "2024-01-18 14:53:02,160 : - PROGRESS: at sentence #20000, processed 127689 words, keeping 21033 word types\n",
      "2024-01-18 14:53:02,170 : - PROGRESS: at sentence #25000, processed 159589 words, keeping 23491 word types\n",
      "2024-01-18 14:53:02,177 : - PROGRESS: at sentence #30000, processed 191554 words, keeping 25494 word types\n",
      "2024-01-18 14:53:02,187 : - PROGRESS: at sentence #35000, processed 223412 words, keeping 27330 word types\n",
      "2024-01-18 14:53:02,204 : - PROGRESS: at sentence #40000, processed 255282 words, keeping 29053 word types\n",
      "2024-01-18 14:53:02,214 : - PROGRESS: at sentence #45000, processed 287297 words, keeping 30606 word types\n",
      "2024-01-18 14:53:02,222 : - PROGRESS: at sentence #50000, processed 319258 words, keeping 31964 word types\n",
      "2024-01-18 14:53:02,231 : - PROGRESS: at sentence #55000, processed 351437 words, keeping 33270 word types\n",
      "2024-01-18 14:53:02,244 : - PROGRESS: at sentence #60000, processed 383579 words, keeping 34520 word types\n",
      "2024-01-18 14:53:02,256 : - PROGRESS: at sentence #65000, processed 415565 words, keeping 35643 word types\n",
      "2024-01-18 14:53:02,265 : - PROGRESS: at sentence #70000, processed 447646 words, keeping 36719 word types\n",
      "2024-01-18 14:53:02,276 : - PROGRESS: at sentence #75000, processed 479568 words, keeping 37802 word types\n",
      "2024-01-18 14:53:02,289 : - PROGRESS: at sentence #80000, processed 511645 words, keeping 38814 word types\n",
      "2024-01-18 14:53:02,297 : - collected 39693 word types from a corpus of 540242 raw words and 84466 sentences\n",
      "2024-01-18 14:53:02,298 : - Creating a fresh vocabulary\n",
      "2024-01-18 14:53:02,410 : - Word2Vec lifecycle event {'msg': 'effective_min_count=5 retains 12924 unique words (32.56% of original 39693, drops 26769)', 'datetime': '2024-01-18T14:53:02.410157', 'gensim': '4.3.0', 'python': '3.11.5 | packaged by Anaconda, Inc. | (main, Sep 11 2023, 13:26:23) [MSC v.1916 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.22621-SP0', 'event': 'prepare_vocab'}\n",
      "2024-01-18 14:53:02,412 : - Word2Vec lifecycle event {'msg': 'effective_min_count=5 leaves 495223 word corpus (91.67% of original 540242, drops 45019)', 'datetime': '2024-01-18T14:53:02.412152', 'gensim': '4.3.0', 'python': '3.11.5 | packaged by Anaconda, Inc. | (main, Sep 11 2023, 13:26:23) [MSC v.1916 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.22621-SP0', 'event': 'prepare_vocab'}\n",
      "2024-01-18 14:53:02,599 : - deleting the raw counts dictionary of 39693 items\n",
      "2024-01-18 14:53:02,602 : - sample=0.001 downsamples 8 most-common words\n",
      "2024-01-18 14:53:02,603 : - Word2Vec lifecycle event {'msg': 'downsampling leaves estimated 486147.7552334345 word corpus (98.2%% of prior 495223)', 'datetime': '2024-01-18T14:53:02.603638', 'gensim': '4.3.0', 'python': '3.11.5 | packaged by Anaconda, Inc. | (main, Sep 11 2023, 13:26:23) [MSC v.1916 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.22621-SP0', 'event': 'prepare_vocab'}\n",
      "2024-01-18 14:53:02,723 : - estimated required memory for 12924 words and 300 dimensions: 37479600 bytes\n",
      "2024-01-18 14:53:02,724 : - resetting layer weights\n",
      "2024-01-18 14:53:02,792 : - Word2Vec lifecycle event {'update': False, 'trim_rule': 'None', 'datetime': '2024-01-18T14:53:02.792114', 'gensim': '4.3.0', 'python': '3.11.5 | packaged by Anaconda, Inc. | (main, Sep 11 2023, 13:26:23) [MSC v.1916 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.22621-SP0', 'event': 'build_vocab'}\n"
     ]
    }
   ],
   "source": [
    "# Configurando o log do modelo Word2Vec.\n",
    "logging.basicConfig(format='%(asctime)s : - %(message)s', level=logging.INFO)\n",
    "\n",
    "# Criando um modelo Word2Vec\n",
    "w2v_modelo = Word2Vec(sg=0, # CBOW\n",
    "                      window=2, # Considerando 2 palavras antes e depois.\n",
    "                      vector_size=300, # Tamanho do vetor do Word2Vec\n",
    "                      min_count=5, # Considerar apenas palavras que aparecem 5 vezes ou mais. \n",
    "                      alpha=0.03, # Taxa de aprendizado.\n",
    "                      min_alpha=0.007) # Taxa de aprendizado (minima)\n",
    "\n",
    "\n",
    "# Criando o vocabulário do modelo Word2Vec.\n",
    "w2v_modelo.build_vocab(lista_lista_tokens, # Lista de lista de tokens de nossos títulos.\n",
    "                       progress_per=5000) # A cada 5000 títulos, imprime uma mensagem de progresso."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Treinamento CBOW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-01-18 14:53:02,802 : - Word2Vec lifecycle event {'msg': 'training model with 3 workers on 12924 vocabulary and 300 features, using sg=0 hs=0 sample=0.001 negative=5 window=2 shrink_windows=True', 'datetime': '2024-01-18T14:53:02.802709', 'gensim': '4.3.0', 'python': '3.11.5 | packaged by Anaconda, Inc. | (main, Sep 11 2023, 13:26:23) [MSC v.1916 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.22621-SP0', 'event': 'train'}\n",
      "2024-01-18 14:53:03,682 : - EPOCH 0: training on 540242 raw words (485990 effective words) took 0.8s, 578190 effective words/s\n",
      "2024-01-18 14:53:04,322 : - EPOCH 1: training on 540242 raw words (486168 effective words) took 0.6s, 772036 effective words/s\n",
      "2024-01-18 14:53:05,083 : - EPOCH 2: training on 540242 raw words (486110 effective words) took 0.8s, 647397 effective words/s\n",
      "2024-01-18 14:53:05,753 : - EPOCH 3: training on 540242 raw words (486069 effective words) took 0.7s, 738953 effective words/s\n",
      "2024-01-18 14:53:06,385 : - EPOCH 4: training on 540242 raw words (486236 effective words) took 0.6s, 781105 effective words/s\n",
      "2024-01-18 14:53:07,021 : - EPOCH 5: training on 540242 raw words (486103 effective words) took 0.6s, 777028 effective words/s\n",
      "2024-01-18 14:53:07,646 : - EPOCH 6: training on 540242 raw words (485994 effective words) took 0.6s, 790502 effective words/s\n",
      "2024-01-18 14:53:08,380 : - EPOCH 7: training on 540242 raw words (486217 effective words) took 0.7s, 670784 effective words/s\n",
      "2024-01-18 14:53:09,026 : - EPOCH 8: training on 540242 raw words (486187 effective words) took 0.6s, 774394 effective words/s\n",
      "2024-01-18 14:53:09,670 : - EPOCH 9: training on 540242 raw words (486172 effective words) took 0.6s, 767821 effective words/s\n",
      "2024-01-18 14:53:10,297 : - EPOCH 10: training on 540242 raw words (486112 effective words) took 0.6s, 786829 effective words/s\n",
      "2024-01-18 14:53:10,896 : - EPOCH 11: training on 540242 raw words (486004 effective words) took 0.6s, 833148 effective words/s\n",
      "2024-01-18 14:53:11,537 : - EPOCH 12: training on 540242 raw words (486158 effective words) took 0.6s, 791847 effective words/s\n",
      "2024-01-18 14:53:12,228 : - EPOCH 13: training on 540242 raw words (486149 effective words) took 0.7s, 722578 effective words/s\n",
      "2024-01-18 14:53:12,831 : - EPOCH 14: training on 540242 raw words (486173 effective words) took 0.6s, 819115 effective words/s\n",
      "2024-01-18 14:53:13,433 : - EPOCH 15: training on 540242 raw words (486190 effective words) took 0.6s, 820885 effective words/s\n",
      "2024-01-18 14:53:14,113 : - EPOCH 16: training on 540242 raw words (486089 effective words) took 0.7s, 726784 effective words/s\n",
      "2024-01-18 14:53:14,744 : - EPOCH 17: training on 540242 raw words (486208 effective words) took 0.6s, 792037 effective words/s\n",
      "2024-01-18 14:53:15,345 : - EPOCH 18: training on 540242 raw words (486186 effective words) took 0.6s, 825631 effective words/s\n",
      "2024-01-18 14:53:15,940 : - EPOCH 19: training on 540242 raw words (486220 effective words) took 0.6s, 832570 effective words/s\n",
      "2024-01-18 14:53:16,551 : - EPOCH 20: training on 540242 raw words (486313 effective words) took 0.6s, 811407 effective words/s\n",
      "2024-01-18 14:53:17,168 : - EPOCH 21: training on 540242 raw words (486183 effective words) took 0.6s, 802273 effective words/s\n",
      "2024-01-18 14:53:17,860 : - EPOCH 22: training on 540242 raw words (486291 effective words) took 0.7s, 713085 effective words/s\n",
      "2024-01-18 14:53:18,465 : - EPOCH 23: training on 540242 raw words (486255 effective words) took 0.6s, 827877 effective words/s\n",
      "2024-01-18 14:53:19,070 : - EPOCH 24: training on 540242 raw words (486142 effective words) took 0.6s, 816736 effective words/s\n",
      "2024-01-18 14:53:19,724 : - EPOCH 25: training on 540242 raw words (486091 effective words) took 0.6s, 753659 effective words/s\n",
      "2024-01-18 14:53:20,390 : - EPOCH 26: training on 540242 raw words (486207 effective words) took 0.6s, 785968 effective words/s\n",
      "2024-01-18 14:53:20,984 : - EPOCH 27: training on 540242 raw words (486187 effective words) took 0.6s, 834708 effective words/s\n",
      "2024-01-18 14:53:21,582 : - EPOCH 28: training on 540242 raw words (486164 effective words) took 0.6s, 826649 effective words/s\n",
      "2024-01-18 14:53:22,183 : - EPOCH 29: training on 540242 raw words (486145 effective words) took 0.6s, 823671 effective words/s\n",
      "2024-01-18 14:53:22,184 : - Word2Vec lifecycle event {'msg': 'training on 16207260 raw words (14584713 effective words) took 19.4s, 752574 effective words/s', 'datetime': '2024-01-18T14:53:22.184187', 'gensim': '4.3.0', 'python': '3.11.5 | packaged by Anaconda, Inc. | (main, Sep 11 2023, 13:26:23) [MSC v.1916 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.22621-SP0', 'event': 'train'}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(14584713, 16207260)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Treinando o modelo Word2Vec.\n",
    "w2v_modelo.train(lista_lista_tokens, # Lista de lista de tokens de nossos títulos.\n",
    "                 total_examples=w2v_modelo.corpus_count, # Número total de exemplos.\n",
    "                 epochs=30) # Número de épocas de treinamento.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Treinamento Skip-gram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-01-18 14:53:22,201 : - Word2Vec lifecycle event {'params': 'Word2Vec<vocab=0, vector_size=300, alpha=0.03>', 'datetime': '2024-01-18T14:53:22.201508', 'gensim': '4.3.0', 'python': '3.11.5 | packaged by Anaconda, Inc. | (main, Sep 11 2023, 13:26:23) [MSC v.1916 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.22621-SP0', 'event': 'created'}\n",
      "2024-01-18 14:53:22,202 : - collecting all words and their counts\n",
      "2024-01-18 14:53:22,203 : - PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "2024-01-18 14:53:22,213 : - PROGRESS: at sentence #5000, processed 31930 words, keeping 10193 word types\n",
      "2024-01-18 14:53:22,222 : - PROGRESS: at sentence #10000, processed 63848 words, keeping 14989 word types\n",
      "2024-01-18 14:53:22,230 : - PROGRESS: at sentence #15000, processed 95753 words, keeping 18279 word types\n",
      "2024-01-18 14:53:22,238 : - PROGRESS: at sentence #20000, processed 127689 words, keeping 21033 word types\n",
      "2024-01-18 14:53:22,248 : - PROGRESS: at sentence #25000, processed 159589 words, keeping 23491 word types\n",
      "2024-01-18 14:53:22,256 : - PROGRESS: at sentence #30000, processed 191554 words, keeping 25494 word types\n",
      "2024-01-18 14:53:22,265 : - PROGRESS: at sentence #35000, processed 223412 words, keeping 27330 word types\n",
      "2024-01-18 14:53:22,274 : - PROGRESS: at sentence #40000, processed 255282 words, keeping 29053 word types\n",
      "2024-01-18 14:53:22,283 : - PROGRESS: at sentence #45000, processed 287297 words, keeping 30606 word types\n",
      "2024-01-18 14:53:22,292 : - PROGRESS: at sentence #50000, processed 319258 words, keeping 31964 word types\n",
      "2024-01-18 14:53:22,300 : - PROGRESS: at sentence #55000, processed 351437 words, keeping 33270 word types\n",
      "2024-01-18 14:53:22,308 : - PROGRESS: at sentence #60000, processed 383579 words, keeping 34520 word types\n",
      "2024-01-18 14:53:22,317 : - PROGRESS: at sentence #65000, processed 415565 words, keeping 35643 word types\n",
      "2024-01-18 14:53:22,325 : - PROGRESS: at sentence #70000, processed 447646 words, keeping 36719 word types\n",
      "2024-01-18 14:53:22,333 : - PROGRESS: at sentence #75000, processed 479568 words, keeping 37802 word types\n",
      "2024-01-18 14:53:22,342 : - PROGRESS: at sentence #80000, processed 511645 words, keeping 38814 word types\n",
      "2024-01-18 14:53:22,349 : - collected 39693 word types from a corpus of 540242 raw words and 84466 sentences\n",
      "2024-01-18 14:53:22,350 : - Creating a fresh vocabulary\n",
      "2024-01-18 14:53:22,406 : - Word2Vec lifecycle event {'msg': 'effective_min_count=5 retains 12924 unique words (32.56% of original 39693, drops 26769)', 'datetime': '2024-01-18T14:53:22.406001', 'gensim': '4.3.0', 'python': '3.11.5 | packaged by Anaconda, Inc. | (main, Sep 11 2023, 13:26:23) [MSC v.1916 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.22621-SP0', 'event': 'prepare_vocab'}\n",
      "2024-01-18 14:53:22,406 : - Word2Vec lifecycle event {'msg': 'effective_min_count=5 leaves 495223 word corpus (91.67% of original 540242, drops 45019)', 'datetime': '2024-01-18T14:53:22.406956', 'gensim': '4.3.0', 'python': '3.11.5 | packaged by Anaconda, Inc. | (main, Sep 11 2023, 13:26:23) [MSC v.1916 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.22621-SP0', 'event': 'prepare_vocab'}\n",
      "2024-01-18 14:53:22,474 : - deleting the raw counts dictionary of 39693 items\n",
      "2024-01-18 14:53:22,476 : - sample=0.001 downsamples 8 most-common words\n",
      "2024-01-18 14:53:22,477 : - Word2Vec lifecycle event {'msg': 'downsampling leaves estimated 486147.7552334345 word corpus (98.2%% of prior 495223)', 'datetime': '2024-01-18T14:53:22.477818', 'gensim': '4.3.0', 'python': '3.11.5 | packaged by Anaconda, Inc. | (main, Sep 11 2023, 13:26:23) [MSC v.1916 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.22621-SP0', 'event': 'prepare_vocab'}\n",
      "2024-01-18 14:53:22,573 : - estimated required memory for 12924 words and 300 dimensions: 37479600 bytes\n",
      "2024-01-18 14:53:22,574 : - resetting layer weights\n",
      "2024-01-18 14:53:22,598 : - Word2Vec lifecycle event {'update': False, 'trim_rule': 'None', 'datetime': '2024-01-18T14:53:22.598448', 'gensim': '4.3.0', 'python': '3.11.5 | packaged by Anaconda, Inc. | (main, Sep 11 2023, 13:26:23) [MSC v.1916 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.22621-SP0', 'event': 'build_vocab'}\n",
      "2024-01-18 14:53:22,599 : - Word2Vec lifecycle event {'msg': 'training model with 3 workers on 12924 vocabulary and 300 features, using sg=1 hs=0 sample=0.001 negative=5 window=5 shrink_windows=True', 'datetime': '2024-01-18T14:53:22.599443', 'gensim': '4.3.0', 'python': '3.11.5 | packaged by Anaconda, Inc. | (main, Sep 11 2023, 13:26:23) [MSC v.1916 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.22621-SP0', 'event': 'train'}\n",
      "2024-01-18 14:53:23,653 : - EPOCH 0 - PROGRESS: at 55.60% examples, 265326 words/s, in_qsize 5, out_qsize 0\n",
      "2024-01-18 14:53:24,318 : - EPOCH 0: training on 540242 raw words (486162 effective words) took 1.7s, 288906 effective words/s\n",
      "2024-01-18 14:53:25,343 : - EPOCH 1 - PROGRESS: at 64.83% examples, 310490 words/s, in_qsize 5, out_qsize 0\n",
      "2024-01-18 14:53:25,869 : - EPOCH 1: training on 540242 raw words (486211 effective words) took 1.5s, 315560 effective words/s\n",
      "2024-01-18 14:53:26,932 : - EPOCH 2 - PROGRESS: at 57.46% examples, 264713 words/s, in_qsize 5, out_qsize 0\n",
      "2024-01-18 14:53:27,728 : - EPOCH 2: training on 540242 raw words (486263 effective words) took 1.8s, 262947 effective words/s\n",
      "2024-01-18 14:53:28,745 : - EPOCH 3 - PROGRESS: at 44.50% examples, 215937 words/s, in_qsize 5, out_qsize 0\n",
      "2024-01-18 14:53:29,770 : - EPOCH 3 - PROGRESS: at 87.00% examples, 208693 words/s, in_qsize 5, out_qsize 0\n",
      "2024-01-18 14:53:30,007 : - EPOCH 3: training on 540242 raw words (486152 effective words) took 2.3s, 214800 effective words/s\n",
      "2024-01-18 14:53:31,039 : - EPOCH 4 - PROGRESS: at 53.76% examples, 256661 words/s, in_qsize 5, out_qsize 0\n",
      "2024-01-18 14:53:31,780 : - EPOCH 4: training on 540242 raw words (486250 effective words) took 1.8s, 276751 effective words/s\n",
      "2024-01-18 14:53:32,808 : - EPOCH 5 - PROGRESS: at 48.21% examples, 229953 words/s, in_qsize 5, out_qsize 0\n",
      "2024-01-18 14:53:33,732 : - EPOCH 5: training on 540242 raw words (486052 effective words) took 1.9s, 250251 effective words/s\n",
      "2024-01-18 14:53:34,786 : - EPOCH 6 - PROGRESS: at 57.46% examples, 267600 words/s, in_qsize 5, out_qsize 0\n",
      "2024-01-18 14:53:35,434 : - EPOCH 6: training on 540242 raw words (486053 effective words) took 1.7s, 287757 effective words/s\n",
      "2024-01-18 14:53:36,480 : - EPOCH 7 - PROGRESS: at 59.32% examples, 278807 words/s, in_qsize 5, out_qsize 0\n",
      "2024-01-18 14:53:37,127 : - EPOCH 7: training on 540242 raw words (486276 effective words) took 1.7s, 289301 effective words/s\n",
      "2024-01-18 14:53:38,150 : - EPOCH 8 - PROGRESS: at 57.46% examples, 276794 words/s, in_qsize 5, out_qsize 0\n",
      "2024-01-18 14:53:38,812 : - EPOCH 8: training on 540242 raw words (486146 effective words) took 1.7s, 291372 effective words/s\n",
      "2024-01-18 14:53:39,842 : - EPOCH 9 - PROGRESS: at 59.32% examples, 282865 words/s, in_qsize 6, out_qsize 0\n",
      "2024-01-18 14:53:40,486 : - EPOCH 9: training on 540242 raw words (486080 effective words) took 1.7s, 292488 effective words/s\n",
      "2024-01-18 14:53:41,512 : - EPOCH 10 - PROGRESS: at 57.46% examples, 275616 words/s, in_qsize 5, out_qsize 0\n",
      "2024-01-18 14:53:42,270 : - EPOCH 10: training on 540242 raw words (486276 effective words) took 1.8s, 274624 effective words/s\n",
      "2024-01-18 14:53:43,284 : - EPOCH 11 - PROGRESS: at 59.32% examples, 287736 words/s, in_qsize 5, out_qsize 0\n",
      "2024-01-18 14:53:43,989 : - EPOCH 11: training on 540242 raw words (486029 effective words) took 1.7s, 284932 effective words/s\n",
      "2024-01-18 14:53:45,058 : - EPOCH 12 - PROGRESS: at 57.46% examples, 264451 words/s, in_qsize 5, out_qsize 0\n",
      "2024-01-18 14:53:45,721 : - EPOCH 12: training on 540242 raw words (486244 effective words) took 1.7s, 283074 effective words/s\n",
      "2024-01-18 14:53:46,750 : - EPOCH 13 - PROGRESS: at 53.76% examples, 257043 words/s, in_qsize 5, out_qsize 0\n",
      "2024-01-18 14:53:47,530 : - EPOCH 13: training on 540242 raw words (486293 effective words) took 1.8s, 270959 effective words/s\n",
      "2024-01-18 14:53:48,571 : - EPOCH 14 - PROGRESS: at 57.46% examples, 271157 words/s, in_qsize 5, out_qsize 0\n",
      "2024-01-18 14:53:49,324 : - EPOCH 14: training on 540242 raw words (486192 effective words) took 1.8s, 272922 effective words/s\n",
      "2024-01-18 14:53:50,339 : - EPOCH 15 - PROGRESS: at 61.16% examples, 296471 words/s, in_qsize 5, out_qsize 0\n",
      "2024-01-18 14:53:50,980 : - EPOCH 15: training on 540242 raw words (486026 effective words) took 1.6s, 296189 effective words/s\n",
      "2024-01-18 14:53:51,997 : - EPOCH 16 - PROGRESS: at 57.46% examples, 278647 words/s, in_qsize 5, out_qsize 0\n",
      "2024-01-18 14:53:52,654 : - EPOCH 16: training on 540242 raw words (486131 effective words) took 1.7s, 293295 effective words/s\n",
      "2024-01-18 14:53:53,726 : - EPOCH 17 - PROGRESS: at 62.98% examples, 291361 words/s, in_qsize 6, out_qsize 0\n",
      "2024-01-18 14:53:54,301 : - EPOCH 17: training on 540242 raw words (486242 effective words) took 1.6s, 299059 effective words/s\n",
      "2024-01-18 14:53:55,318 : - EPOCH 18 - PROGRESS: at 59.32% examples, 286908 words/s, in_qsize 6, out_qsize 0\n",
      "2024-01-18 14:53:55,917 : - EPOCH 18: training on 540242 raw words (486178 effective words) took 1.6s, 303536 effective words/s\n",
      "2024-01-18 14:53:56,982 : - EPOCH 19 - PROGRESS: at 62.98% examples, 290644 words/s, in_qsize 5, out_qsize 0\n",
      "2024-01-18 14:53:57,569 : - EPOCH 19: training on 540242 raw words (486127 effective words) took 1.6s, 296558 effective words/s\n",
      "2024-01-18 14:53:58,584 : - EPOCH 20 - PROGRESS: at 57.46% examples, 278459 words/s, in_qsize 6, out_qsize 0\n",
      "2024-01-18 14:53:59,239 : - EPOCH 20: training on 540242 raw words (486113 effective words) took 1.7s, 293666 effective words/s\n",
      "2024-01-18 14:54:00,263 : - EPOCH 21 - PROGRESS: at 61.16% examples, 293913 words/s, in_qsize 5, out_qsize 0\n",
      "2024-01-18 14:54:00,859 : - EPOCH 21: training on 540242 raw words (486218 effective words) took 1.6s, 302596 effective words/s\n",
      "2024-01-18 14:54:01,887 : - EPOCH 22 - PROGRESS: at 61.16% examples, 292453 words/s, in_qsize 6, out_qsize 0\n",
      "2024-01-18 14:54:02,493 : - EPOCH 22: training on 540242 raw words (486196 effective words) took 1.6s, 300062 effective words/s\n",
      "2024-01-18 14:54:03,560 : - EPOCH 23 - PROGRESS: at 62.98% examples, 290386 words/s, in_qsize 5, out_qsize 0\n",
      "2024-01-18 14:54:04,125 : - EPOCH 23: training on 540242 raw words (486097 effective words) took 1.6s, 300214 effective words/s\n",
      "2024-01-18 14:54:05,147 : - EPOCH 24 - PROGRESS: at 57.46% examples, 276729 words/s, in_qsize 5, out_qsize 0\n",
      "2024-01-18 14:54:05,914 : - EPOCH 24: training on 540242 raw words (486038 effective words) took 1.8s, 273848 effective words/s\n",
      "2024-01-18 14:54:06,938 : - EPOCH 25 - PROGRESS: at 59.29% examples, 284488 words/s, in_qsize 5, out_qsize 0\n",
      "2024-01-18 14:54:07,536 : - EPOCH 25: training on 540242 raw words (486172 effective words) took 1.6s, 301922 effective words/s\n",
      "2024-01-18 14:54:08,565 : - EPOCH 26 - PROGRESS: at 62.98% examples, 301117 words/s, in_qsize 5, out_qsize 0\n",
      "2024-01-18 14:54:09,116 : - EPOCH 26: training on 540242 raw words (486199 effective words) took 1.6s, 310448 effective words/s\n",
      "2024-01-18 14:54:10,192 : - EPOCH 27 - PROGRESS: at 64.83% examples, 296228 words/s, in_qsize 5, out_qsize 0\n",
      "2024-01-18 14:54:10,707 : - EPOCH 27: training on 540242 raw words (486304 effective words) took 1.6s, 308039 effective words/s\n",
      "2024-01-18 14:54:11,747 : - EPOCH 28 - PROGRESS: at 62.98% examples, 298923 words/s, in_qsize 5, out_qsize 0\n",
      "2024-01-18 14:54:12,267 : - EPOCH 28: training on 540242 raw words (486272 effective words) took 1.5s, 314962 effective words/s\n",
      "2024-01-18 14:54:13,283 : - EPOCH 29 - PROGRESS: at 64.83% examples, 314032 words/s, in_qsize 6, out_qsize 0\n",
      "2024-01-18 14:54:13,793 : - EPOCH 29: training on 540242 raw words (486107 effective words) took 1.5s, 321408 effective words/s\n",
      "2024-01-18 14:54:13,794 : - Word2Vec lifecycle event {'msg': 'training on 16207260 raw words (14585099 effective words) took 51.2s, 284895 effective words/s', 'datetime': '2024-01-18T14:54:13.794897', 'gensim': '4.3.0', 'python': '3.11.5 | packaged by Anaconda, Inc. | (main, Sep 11 2023, 13:26:23) [MSC v.1916 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.22621-SP0', 'event': 'train'}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(14585099, 16207260)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Criando um modelo Word2Vec - Skip-gram\n",
    "w2v_modelo_sg = Word2Vec(sg=1, # Skip-gram\n",
    "                      window=5, # Considerando 5 palavras antes e depois.\n",
    "                      vector_size=300, # Tamanho do vetor do Word2Vec\n",
    "                      min_count=5, # Considerar apenas palavras que aparecem 5 vezes ou mais. \n",
    "                      alpha=0.03, # Taxa de aprendizado.\n",
    "                      min_alpha=0.007) # Taxa de aprendizado (minima)\n",
    "\n",
    "\n",
    "# Criando o vocabulário do modelo Word2Vec.\n",
    "w2v_modelo_sg.build_vocab(lista_lista_tokens, # Lista de lista de tokens de nossos títulos.\n",
    "                       progress_per=5000) # A cada 5000 títulos, imprime uma mensagem de progresso.\n",
    "\n",
    "# Treinando o modelo Word2Vec.\n",
    "w2v_modelo_sg.train(lista_lista_tokens, # Lista de lista de tokens de nossos títulos.\n",
    "                 total_examples=w2v_modelo_sg.corpus_count, # Número total de exemplos.\n",
    "                 epochs=30) # Número de épocas de treinamento.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Salvando os modelos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-01-18 14:54:13,822 : - storing 12924x300 projection weights into Dados/modelo_cbow.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-01-18 14:54:18,344 : - storing 12924x300 projection weights into Dados/modelo_skipgram.txt\n"
     ]
    }
   ],
   "source": [
    "w2v_modelo.wv.save_word2vec_format('Dados/modelo_cbow.txt', binary=False)\n",
    "w2v_modelo_sg.wv.save_word2vec_format('Dados/modelo_skipgram.txt', binary=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Criando classificador"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-01-18 18:11:11,868 : - loading projection weights from Dados/modelo_cbow.txt\n",
      "2024-01-18 18:11:17,418 : - KeyedVectors lifecycle event {'msg': 'loaded (12924, 300) matrix of type float32 from Dados/modelo_cbow.txt', 'binary': False, 'encoding': 'utf8', 'datetime': '2024-01-18T18:11:17.341589', 'gensim': '4.3.0', 'python': '3.11.5 | packaged by Anaconda, Inc. | (main, Sep 11 2023, 13:26:23) [MSC v.1916 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.22621-SP0', 'event': 'load_word2vec_format'}\n",
      "2024-01-18 18:11:17,437 : - loading projection weights from Dados/modelo_skipgram.txt\n",
      "2024-01-18 18:11:21,703 : - KeyedVectors lifecycle event {'msg': 'loaded (12924, 300) matrix of type float32 from Dados/modelo_skipgram.txt', 'binary': False, 'encoding': 'utf8', 'datetime': '2024-01-18T18:11:21.703816', 'gensim': '4.3.0', 'python': '3.11.5 | packaged by Anaconda, Inc. | (main, Sep 11 2023, 13:26:23) [MSC v.1916 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.22621-SP0', 'event': 'load_word2vec_format'}\n"
     ]
    }
   ],
   "source": [
    "# Carregando o modelo Word2Vec - Skip-gram\n",
    "w2v_modelo_cbow = KeyedVectors.load_word2vec_format('Dados/modelo_cbow.txt')\n",
    "\n",
    "# Carregando o modelo Word2Vec - Skip-gram\n",
    "w2v_modelo_sg = KeyedVectors.load_word2vec_format('Dados/modelo_skipgram.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = sp.load('pt_core_news_sm', disable=['parser', 'ner', 'tagger', 'textcat'])\n",
    "\n",
    "# Criando uma função para tratamento de texto.\n",
    "def tokenizador(texto):\n",
    "    doc = nlp(texto)\n",
    "    tokens_validos = []\n",
    "    for token in doc:\n",
    "        e_valido = not token.is_stop and token.is_alpha # Token é válido se não for stopword e for alfabético\n",
    "        if e_valido: # Se for token válido, adiciona na lista de tokens válidos.\n",
    "            tokens_validos.append(token.text.lower())\n",
    "\n",
    "    return tokens_validos\n",
    "\n",
    "texto = 'Um texto qualquer para ser tratado.'\n",
    "tokens = tokenizador(texto)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Criando uma função para vetorizar o texto.\n",
    "def combinacao_de_vetores_por_soma(palavras, modelo):\n",
    "    # definindo um vetor de tamanho 300 com valores 0.\n",
    "    vetor_resultante = np.zeros(300)\n",
    "\n",
    "    # para cada palavra-numero no texto, soma o vetor da palavra ao vetor_resultante.\n",
    "    for pn in palavras:\n",
    "        try:\n",
    "            vetor_resultante += modelo.get_vector(pn)    \n",
    "        except KeyError:\n",
    "            pass \n",
    "        \n",
    "    return vetor_resultante\n",
    "\n",
    "vetor_texto = combinacao_de_vetores_por_soma(tokens, w2v_modelo_cbow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(90000, 300)\n",
      "(20513, 300)\n"
     ]
    }
   ],
   "source": [
    "# Criando uma função para vetorizar os dados de treino e teste.\n",
    "def matriz_vetores(textos, modelo):\n",
    "    matriz = np.zeros((len(textos), 300))\n",
    "\n",
    "    for i in range(len(textos)):\n",
    "        palavras = tokenizador(textos.iloc[i])\n",
    "        matriz[i] = combinacao_de_vetores_por_soma(palavras, modelo)\n",
    "\n",
    "    return matriz\n",
    "\n",
    "# Vetorizando os dados de treino e teste.\n",
    "matriz_vetores_treino_cbow = matriz_vetores(dados_treino.title, w2v_modelo_cbow)\n",
    "matriz_vetores_teste_cbow = matriz_vetores(dados_teste.title, w2v_modelo_cbow)\n",
    "\n",
    "print(matriz_vetores_treino_cbow.shape)\n",
    "print(matriz_vetores_teste_cbow.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Criando modelo de Regressão Logística"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "     colunas       0.80      0.71      0.75      6103\n",
      "   cotidiano       0.64      0.80      0.71      1698\n",
      "     esporte       0.93      0.86      0.89      4663\n",
      "   ilustrada       0.13      0.83      0.22       131\n",
      "     mercado       0.84      0.78      0.81      5867\n",
      "       mundo       0.74      0.84      0.79      2051\n",
      "\n",
      "    accuracy                           0.79     20513\n",
      "   macro avg       0.68      0.80      0.70     20513\n",
      "weighted avg       0.82      0.79      0.80     20513\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Treinando um modelo de Regressão Logística.\n",
    "def classificador(modelo, X_treino, X_teste, y_treino, y_teste):\n",
    "    lr = LogisticRegression(max_iter=800)\n",
    "    lr.fit(X_treino, y_treino)\n",
    "    categorias = lr.predict(X_teste)\n",
    "    resultados = classification_report(y_teste, categorias)\n",
    "    print(resultados)\n",
    "    return lr\n",
    "\n",
    "lr_cbow = classificador(w2v_modelo_cbow, matriz_vetores_treino_cbow, matriz_vetores_teste_cbow, dados_treino.category, dados_teste.category)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
