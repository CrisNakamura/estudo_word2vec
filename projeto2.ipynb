{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bibliotecas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import spacy as sp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Carregando os dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dados_treino = pd.read_csv('Dados/treino.csv')\n",
    "dados_teste = pd.read_csv('Dados/teste.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pré-processamento com Spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Carregando o modelo de lingua portuguesa.\n",
    "nlp = sp.load('pt_core_news_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Criando um gerador de textos para tratamento.\n",
    "textos_para_tratamento = (titulos.lower() for titulos in dados_treino['title'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Criando uma função para tratamento de texto.\n",
    "def tratatamento_texto(doc):\n",
    "    tokens_validos = []\n",
    "    for token in doc:\n",
    "        e_valido = not token.is_stop and token.is_alpha # Token é válido se não for stopword e for alfabético\n",
    "        if e_valido: # Se for token válido, adiciona na lista de tokens válidos.\n",
    "            tokens_validos.append(token.text)\n",
    "\n",
    "    if len(tokens_validos) > 2: # Se o texto tiver mais de 2 tokens válidos, retorna o texto tratado.\n",
    "        return ' '.join(tokens_validos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "texto = 'Um texto qualquer para ser tratado.'\n",
    "doc = nlp(texto)\n",
    "\n",
    "# Criando uma lista de textos tratados.\n",
    "textos_tratados = [tratatamento_texto(doc) for doc in nlp.pipe(textos_para_tratamento,\n",
    "                                                                batch_size=1000,\n",
    "                                                                n_process=-1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>titulo</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>polêmica marine le pen abomina negacionistas h...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>macron le pen turno frança revés siglas tradic...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>apesar larga vitória legislativas macron terá ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>governo antecipa balanço alckmin anuncia queda...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>queda maio atividade econômica sobe junho bc</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              titulo\n",
       "0  polêmica marine le pen abomina negacionistas h...\n",
       "1  macron le pen turno frança revés siglas tradic...\n",
       "2  apesar larga vitória legislativas macron terá ...\n",
       "3  governo antecipa balanço alckmin anuncia queda...\n",
       "4       queda maio atividade econômica sobe junho bc"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Criando um dataframe com os textos tratados.\n",
    "titulos_tratados = pd.DataFrame({'titulo': textos_tratados})\n",
    "titulos_tratados.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Criando o modelo Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "\n",
    "# Criando um modelo Word2Vec\n",
    "w2v_modelo = Word2Vec(sg=0, # CBOW\n",
    "                      window=2, # Considerando 2 palavras antes e depois.\n",
    "                      vector_size=300, # Tamanho do vetor do Word2Vec\n",
    "                      min_count=5, # Considerar apenas palavras que aparecem 5 vezes ou mais. \n",
    "                      alpha=0.03, # Taxa de aprendizado.\n",
    "                      min_alpha=0.007) # Taxa de aprendizado (minima)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Construindo o vocabulário do modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "90000\n",
      "84466\n"
     ]
    }
   ],
   "source": [
    "# Imprimindo o tamanho do vocabulário antes do tratamento.\n",
    "print(len(titulos_tratados))\n",
    "\n",
    "# Removendo linhas com valores nulos e duplicados.\n",
    "titulos_tratados = titulos_tratados.dropna().drop_duplicates()\n",
    "\n",
    "# Imprimindo o tamanho do vocabulário após o tratamento.\n",
    "print(len(titulos_tratados))\n",
    "\n",
    "# Criando uma lista de listas de tokens.\n",
    "lista_lista_tokens = [titulo.split(' ') for titulo in titulos_tratados['titulo']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-01-17 20:37:58,421 : - Word2Vec lifecycle event {'params': 'Word2Vec<vocab=0, vector_size=300, alpha=0.03>', 'datetime': '2024-01-17T20:37:58.421768', 'gensim': '4.3.0', 'python': '3.11.5 | packaged by Anaconda, Inc. | (main, Sep 11 2023, 13:26:23) [MSC v.1916 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.22621-SP0', 'event': 'created'}\n",
      "2024-01-17 20:37:58,422 : - collecting all words and their counts\n",
      "2024-01-17 20:37:58,423 : - PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "2024-01-17 20:37:58,432 : - PROGRESS: at sentence #5000, processed 31930 words, keeping 10193 word types\n",
      "2024-01-17 20:37:58,448 : - PROGRESS: at sentence #10000, processed 63848 words, keeping 14989 word types\n",
      "2024-01-17 20:37:58,464 : - PROGRESS: at sentence #15000, processed 95753 words, keeping 18279 word types\n",
      "2024-01-17 20:37:58,482 : - PROGRESS: at sentence #20000, processed 127689 words, keeping 21033 word types\n",
      "2024-01-17 20:37:58,502 : - PROGRESS: at sentence #25000, processed 159589 words, keeping 23491 word types\n",
      "2024-01-17 20:37:58,531 : - PROGRESS: at sentence #30000, processed 191554 words, keeping 25494 word types\n",
      "2024-01-17 20:37:58,553 : - PROGRESS: at sentence #35000, processed 223412 words, keeping 27330 word types\n",
      "2024-01-17 20:37:58,593 : - PROGRESS: at sentence #40000, processed 255282 words, keeping 29053 word types\n",
      "2024-01-17 20:37:58,616 : - PROGRESS: at sentence #45000, processed 287297 words, keeping 30606 word types\n",
      "2024-01-17 20:37:58,632 : - PROGRESS: at sentence #50000, processed 319258 words, keeping 31964 word types\n",
      "2024-01-17 20:37:58,644 : - PROGRESS: at sentence #55000, processed 351437 words, keeping 33270 word types\n",
      "2024-01-17 20:37:58,653 : - PROGRESS: at sentence #60000, processed 383579 words, keeping 34520 word types\n",
      "2024-01-17 20:37:58,663 : - PROGRESS: at sentence #65000, processed 415565 words, keeping 35643 word types\n",
      "2024-01-17 20:37:58,672 : - PROGRESS: at sentence #70000, processed 447646 words, keeping 36719 word types\n",
      "2024-01-17 20:37:58,680 : - PROGRESS: at sentence #75000, processed 479568 words, keeping 37802 word types\n",
      "2024-01-17 20:37:58,692 : - PROGRESS: at sentence #80000, processed 511645 words, keeping 38814 word types\n",
      "2024-01-17 20:37:58,701 : - collected 39693 word types from a corpus of 540242 raw words and 84466 sentences\n",
      "2024-01-17 20:37:58,701 : - Creating a fresh vocabulary\n",
      "2024-01-17 20:37:58,748 : - Word2Vec lifecycle event {'msg': 'effective_min_count=5 retains 12924 unique words (32.56% of original 39693, drops 26769)', 'datetime': '2024-01-17T20:37:58.748783', 'gensim': '4.3.0', 'python': '3.11.5 | packaged by Anaconda, Inc. | (main, Sep 11 2023, 13:26:23) [MSC v.1916 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.22621-SP0', 'event': 'prepare_vocab'}\n",
      "2024-01-17 20:37:58,749 : - Word2Vec lifecycle event {'msg': 'effective_min_count=5 leaves 495223 word corpus (91.67% of original 540242, drops 45019)', 'datetime': '2024-01-17T20:37:58.749780', 'gensim': '4.3.0', 'python': '3.11.5 | packaged by Anaconda, Inc. | (main, Sep 11 2023, 13:26:23) [MSC v.1916 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.22621-SP0', 'event': 'prepare_vocab'}\n",
      "2024-01-17 20:37:58,863 : - deleting the raw counts dictionary of 39693 items\n",
      "2024-01-17 20:37:58,865 : - sample=0.001 downsamples 8 most-common words\n",
      "2024-01-17 20:37:58,895 : - Word2Vec lifecycle event {'msg': 'downsampling leaves estimated 486147.7552334345 word corpus (98.2%% of prior 495223)', 'datetime': '2024-01-17T20:37:58.894039', 'gensim': '4.3.0', 'python': '3.11.5 | packaged by Anaconda, Inc. | (main, Sep 11 2023, 13:26:23) [MSC v.1916 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.22621-SP0', 'event': 'prepare_vocab'}\n",
      "2024-01-17 20:37:59,024 : - estimated required memory for 12924 words and 300 dimensions: 37479600 bytes\n",
      "2024-01-17 20:37:59,025 : - resetting layer weights\n",
      "2024-01-17 20:37:59,077 : - Word2Vec lifecycle event {'update': False, 'trim_rule': 'None', 'datetime': '2024-01-17T20:37:59.077045', 'gensim': '4.3.0', 'python': '3.11.5 | packaged by Anaconda, Inc. | (main, Sep 11 2023, 13:26:23) [MSC v.1916 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.22621-SP0', 'event': 'build_vocab'}\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "\n",
    "# Configurando o log do modelo Word2Vec.\n",
    "logging.basicConfig(format='%(asctime)s : - %(message)s', level=logging.INFO)\n",
    "\n",
    "# Criando um modelo Word2Vec\n",
    "w2v_modelo = Word2Vec(sg=0, # CBOW\n",
    "                      window=2, # Considerando 2 palavras antes e depois.\n",
    "                      vector_size=300, # Tamanho do vetor do Word2Vec\n",
    "                      min_count=5, # Considerar apenas palavras que aparecem 5 vezes ou mais. \n",
    "                      alpha=0.03, # Taxa de aprendizado.\n",
    "                      min_alpha=0.007) # Taxa de aprendizado (minima)\n",
    "\n",
    "\n",
    "# Criando o vocabulário do modelo Word2Vec.\n",
    "w2v_modelo.build_vocab(lista_lista_tokens, # Lista de lista de tokens de nossos títulos.\n",
    "                       progress_per=5000) # A cada 5000 títulos, imprime uma mensagem de progresso."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Treinamento CBOW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-01-17 20:37:59,103 : - Word2Vec lifecycle event {'msg': 'training model with 3 workers on 12924 vocabulary and 300 features, using sg=0 hs=0 sample=0.001 negative=5 window=2 shrink_windows=True', 'datetime': '2024-01-17T20:37:59.103546', 'gensim': '4.3.0', 'python': '3.11.5 | packaged by Anaconda, Inc. | (main, Sep 11 2023, 13:26:23) [MSC v.1916 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.22621-SP0', 'event': 'train'}\n",
      "2024-01-17 20:38:00,076 : - EPOCH 0: training on 540242 raw words (485990 effective words) took 0.9s, 525745 effective words/s\n",
      "2024-01-17 20:38:00,872 : - EPOCH 1: training on 540242 raw words (486141 effective words) took 0.8s, 646758 effective words/s\n",
      "2024-01-17 20:38:01,588 : - EPOCH 2: training on 540242 raw words (486137 effective words) took 0.7s, 690929 effective words/s\n",
      "2024-01-17 20:38:02,353 : - EPOCH 3: training on 540242 raw words (486260 effective words) took 0.8s, 645786 effective words/s\n",
      "2024-01-17 20:38:03,030 : - EPOCH 4: training on 540242 raw words (486162 effective words) took 0.7s, 728522 effective words/s\n",
      "2024-01-17 20:38:03,713 : - EPOCH 5: training on 540242 raw words (486119 effective words) took 0.7s, 723721 effective words/s\n",
      "2024-01-17 20:38:04,387 : - EPOCH 6: training on 540242 raw words (486040 effective words) took 0.7s, 733942 effective words/s\n",
      "2024-01-17 20:38:05,071 : - EPOCH 7: training on 540242 raw words (486183 effective words) took 0.7s, 721514 effective words/s\n",
      "2024-01-17 20:38:05,756 : - EPOCH 8: training on 540242 raw words (486036 effective words) took 0.7s, 720543 effective words/s\n",
      "2024-01-17 20:38:06,467 : - EPOCH 9: training on 540242 raw words (486039 effective words) took 0.7s, 694597 effective words/s\n",
      "2024-01-17 20:38:07,189 : - EPOCH 10: training on 540242 raw words (486150 effective words) took 0.7s, 683498 effective words/s\n",
      "2024-01-17 20:38:07,935 : - EPOCH 11: training on 540242 raw words (486062 effective words) took 0.7s, 660898 effective words/s\n",
      "2024-01-17 20:38:08,664 : - EPOCH 12: training on 540242 raw words (486081 effective words) took 0.7s, 679379 effective words/s\n",
      "2024-01-17 20:38:09,335 : - EPOCH 13: training on 540242 raw words (486076 effective words) took 0.7s, 734998 effective words/s\n",
      "2024-01-17 20:38:09,994 : - EPOCH 14: training on 540242 raw words (486149 effective words) took 0.6s, 754090 effective words/s\n",
      "2024-01-17 20:38:10,664 : - EPOCH 15: training on 540242 raw words (486140 effective words) took 0.7s, 739917 effective words/s\n",
      "2024-01-17 20:38:11,386 : - EPOCH 16: training on 540242 raw words (486161 effective words) took 0.7s, 683135 effective words/s\n",
      "2024-01-17 20:38:12,120 : - EPOCH 17: training on 540242 raw words (486168 effective words) took 0.7s, 672621 effective words/s\n",
      "2024-01-17 20:38:12,779 : - EPOCH 18: training on 540242 raw words (486141 effective words) took 0.6s, 750688 effective words/s\n",
      "2024-01-17 20:38:13,446 : - EPOCH 19: training on 540242 raw words (486069 effective words) took 0.7s, 740019 effective words/s\n",
      "2024-01-17 20:38:14,195 : - EPOCH 20: training on 540242 raw words (486135 effective words) took 0.7s, 683484 effective words/s\n",
      "2024-01-17 20:38:14,862 : - EPOCH 21: training on 540242 raw words (486118 effective words) took 0.7s, 739714 effective words/s\n",
      "2024-01-17 20:38:15,518 : - EPOCH 22: training on 540242 raw words (486200 effective words) took 0.6s, 756258 effective words/s\n",
      "2024-01-17 20:38:16,169 : - EPOCH 23: training on 540242 raw words (486191 effective words) took 0.6s, 762899 effective words/s\n",
      "2024-01-17 20:38:16,817 : - EPOCH 24: training on 540242 raw words (486202 effective words) took 0.6s, 762868 effective words/s\n",
      "2024-01-17 20:38:17,475 : - EPOCH 25: training on 540242 raw words (486104 effective words) took 0.6s, 751281 effective words/s\n",
      "2024-01-17 20:38:18,124 : - EPOCH 26: training on 540242 raw words (486110 effective words) took 0.6s, 774387 effective words/s\n",
      "2024-01-17 20:38:18,792 : - EPOCH 27: training on 540242 raw words (486164 effective words) took 0.7s, 740263 effective words/s\n",
      "2024-01-17 20:38:19,528 : - EPOCH 28: training on 540242 raw words (486168 effective words) took 0.7s, 670124 effective words/s\n",
      "2024-01-17 20:38:20,199 : - EPOCH 29: training on 540242 raw words (486159 effective words) took 0.7s, 741791 effective words/s\n",
      "2024-01-17 20:38:20,200 : - Word2Vec lifecycle event {'msg': 'training on 16207260 raw words (14583855 effective words) took 21.1s, 691356 effective words/s', 'datetime': '2024-01-17T20:38:20.200337', 'gensim': '4.3.0', 'python': '3.11.5 | packaged by Anaconda, Inc. | (main, Sep 11 2023, 13:26:23) [MSC v.1916 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.22621-SP0', 'event': 'train'}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(14583855, 16207260)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Treinando o modelo Word2Vec.\n",
    "w2v_modelo.train(lista_lista_tokens, # Lista de lista de tokens de nossos títulos.\n",
    "                 total_examples=w2v_modelo.corpus_count, # Número total de exemplos.\n",
    "                 epochs=30) # Número de épocas de treinamento.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Treinamento Skip-gram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-01-17 20:38:20,222 : - Word2Vec lifecycle event {'params': 'Word2Vec<vocab=0, vector_size=300, alpha=0.03>', 'datetime': '2024-01-17T20:38:20.222700', 'gensim': '4.3.0', 'python': '3.11.5 | packaged by Anaconda, Inc. | (main, Sep 11 2023, 13:26:23) [MSC v.1916 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.22621-SP0', 'event': 'created'}\n",
      "2024-01-17 20:38:20,224 : - collecting all words and their counts\n",
      "2024-01-17 20:38:20,225 : - PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "2024-01-17 20:38:20,242 : - PROGRESS: at sentence #5000, processed 31930 words, keeping 10193 word types\n",
      "2024-01-17 20:38:20,258 : - PROGRESS: at sentence #10000, processed 63848 words, keeping 14989 word types\n",
      "2024-01-17 20:38:20,267 : - PROGRESS: at sentence #15000, processed 95753 words, keeping 18279 word types\n",
      "2024-01-17 20:38:20,276 : - PROGRESS: at sentence #20000, processed 127689 words, keeping 21033 word types\n",
      "2024-01-17 20:38:20,286 : - PROGRESS: at sentence #25000, processed 159589 words, keeping 23491 word types\n",
      "2024-01-17 20:38:20,295 : - PROGRESS: at sentence #30000, processed 191554 words, keeping 25494 word types\n",
      "2024-01-17 20:38:20,304 : - PROGRESS: at sentence #35000, processed 223412 words, keeping 27330 word types\n",
      "2024-01-17 20:38:20,315 : - PROGRESS: at sentence #40000, processed 255282 words, keeping 29053 word types\n",
      "2024-01-17 20:38:20,332 : - PROGRESS: at sentence #45000, processed 287297 words, keeping 30606 word types\n",
      "2024-01-17 20:38:20,357 : - PROGRESS: at sentence #50000, processed 319258 words, keeping 31964 word types\n",
      "2024-01-17 20:38:20,381 : - PROGRESS: at sentence #55000, processed 351437 words, keeping 33270 word types\n",
      "2024-01-17 20:38:20,397 : - PROGRESS: at sentence #60000, processed 383579 words, keeping 34520 word types\n",
      "2024-01-17 20:38:20,409 : - PROGRESS: at sentence #65000, processed 415565 words, keeping 35643 word types\n",
      "2024-01-17 20:38:20,421 : - PROGRESS: at sentence #70000, processed 447646 words, keeping 36719 word types\n",
      "2024-01-17 20:38:20,432 : - PROGRESS: at sentence #75000, processed 479568 words, keeping 37802 word types\n",
      "2024-01-17 20:38:20,445 : - PROGRESS: at sentence #80000, processed 511645 words, keeping 38814 word types\n",
      "2024-01-17 20:38:20,456 : - collected 39693 word types from a corpus of 540242 raw words and 84466 sentences\n",
      "2024-01-17 20:38:20,456 : - Creating a fresh vocabulary\n",
      "2024-01-17 20:38:20,559 : - Word2Vec lifecycle event {'msg': 'effective_min_count=5 retains 12924 unique words (32.56% of original 39693, drops 26769)', 'datetime': '2024-01-17T20:38:20.559801', 'gensim': '4.3.0', 'python': '3.11.5 | packaged by Anaconda, Inc. | (main, Sep 11 2023, 13:26:23) [MSC v.1916 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.22621-SP0', 'event': 'prepare_vocab'}\n",
      "2024-01-17 20:38:20,560 : - Word2Vec lifecycle event {'msg': 'effective_min_count=5 leaves 495223 word corpus (91.67% of original 540242, drops 45019)', 'datetime': '2024-01-17T20:38:20.560798', 'gensim': '4.3.0', 'python': '3.11.5 | packaged by Anaconda, Inc. | (main, Sep 11 2023, 13:26:23) [MSC v.1916 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.22621-SP0', 'event': 'prepare_vocab'}\n",
      "2024-01-17 20:38:20,656 : - deleting the raw counts dictionary of 39693 items\n",
      "2024-01-17 20:38:20,658 : - sample=0.001 downsamples 8 most-common words\n",
      "2024-01-17 20:38:20,659 : - Word2Vec lifecycle event {'msg': 'downsampling leaves estimated 486147.7552334345 word corpus (98.2%% of prior 495223)', 'datetime': '2024-01-17T20:38:20.659533', 'gensim': '4.3.0', 'python': '3.11.5 | packaged by Anaconda, Inc. | (main, Sep 11 2023, 13:26:23) [MSC v.1916 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.22621-SP0', 'event': 'prepare_vocab'}\n",
      "2024-01-17 20:38:20,771 : - estimated required memory for 12924 words and 300 dimensions: 37479600 bytes\n",
      "2024-01-17 20:38:20,772 : - resetting layer weights\n",
      "2024-01-17 20:38:20,792 : - Word2Vec lifecycle event {'update': False, 'trim_rule': 'None', 'datetime': '2024-01-17T20:38:20.792677', 'gensim': '4.3.0', 'python': '3.11.5 | packaged by Anaconda, Inc. | (main, Sep 11 2023, 13:26:23) [MSC v.1916 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.22621-SP0', 'event': 'build_vocab'}\n",
      "2024-01-17 20:38:20,793 : - Word2Vec lifecycle event {'msg': 'training model with 3 workers on 12924 vocabulary and 300 features, using sg=1 hs=0 sample=0.001 negative=5 window=5 shrink_windows=True', 'datetime': '2024-01-17T20:38:20.793674', 'gensim': '4.3.0', 'python': '3.11.5 | packaged by Anaconda, Inc. | (main, Sep 11 2023, 13:26:23) [MSC v.1916 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.22621-SP0', 'event': 'train'}\n",
      "2024-01-17 20:38:21,845 : - EPOCH 0 - PROGRESS: at 46.35% examples, 216357 words/s, in_qsize 5, out_qsize 0\n",
      "2024-01-17 20:38:22,706 : - EPOCH 0: training on 540242 raw words (486059 effective words) took 1.9s, 255646 effective words/s\n",
      "2024-01-17 20:38:23,765 : - EPOCH 1 - PROGRESS: at 51.91% examples, 240986 words/s, in_qsize 4, out_qsize 1\n",
      "2024-01-17 20:38:24,638 : - EPOCH 1: training on 540242 raw words (486189 effective words) took 1.9s, 253457 effective words/s\n",
      "2024-01-17 20:38:25,664 : - EPOCH 2 - PROGRESS: at 51.91% examples, 248499 words/s, in_qsize 5, out_qsize 0\n",
      "2024-01-17 20:38:26,616 : - EPOCH 2: training on 540242 raw words (486058 effective words) took 2.0s, 247254 effective words/s\n",
      "2024-01-17 20:38:27,665 : - EPOCH 3 - PROGRESS: at 53.76% examples, 252266 words/s, in_qsize 5, out_qsize 0\n",
      "2024-01-17 20:38:28,453 : - EPOCH 3: training on 540242 raw words (486135 effective words) took 1.8s, 266636 effective words/s\n",
      "2024-01-17 20:38:29,511 : - EPOCH 4 - PROGRESS: at 51.91% examples, 241309 words/s, in_qsize 6, out_qsize 0\n",
      "2024-01-17 20:38:30,456 : - EPOCH 4: training on 540242 raw words (486248 effective words) took 2.0s, 244632 effective words/s\n",
      "2024-01-17 20:38:31,505 : - EPOCH 5 - PROGRESS: at 51.91% examples, 243477 words/s, in_qsize 6, out_qsize 0\n",
      "2024-01-17 20:38:32,318 : - EPOCH 5: training on 540242 raw words (486220 effective words) took 1.8s, 262963 effective words/s\n",
      "2024-01-17 20:38:33,359 : - EPOCH 6 - PROGRESS: at 53.76% examples, 254641 words/s, in_qsize 5, out_qsize 0\n",
      "2024-01-17 20:38:34,250 : - EPOCH 6: training on 540242 raw words (486112 effective words) took 1.9s, 253679 effective words/s\n",
      "2024-01-17 20:38:35,270 : - EPOCH 7 - PROGRESS: at 55.60% examples, 268490 words/s, in_qsize 5, out_qsize 0\n",
      "2024-01-17 20:38:36,038 : - EPOCH 7: training on 540242 raw words (486186 effective words) took 1.8s, 274016 effective words/s\n",
      "2024-01-17 20:38:37,111 : - EPOCH 8 - PROGRESS: at 57.46% examples, 262801 words/s, in_qsize 6, out_qsize 0\n",
      "2024-01-17 20:38:37,836 : - EPOCH 8: training on 540242 raw words (486138 effective words) took 1.8s, 272188 effective words/s\n",
      "2024-01-17 20:38:38,904 : - EPOCH 9 - PROGRESS: at 57.46% examples, 264010 words/s, in_qsize 6, out_qsize 0\n",
      "2024-01-17 20:38:39,663 : - EPOCH 9: training on 540242 raw words (486309 effective words) took 1.8s, 267996 effective words/s\n",
      "2024-01-17 20:38:40,692 : - EPOCH 10 - PROGRESS: at 46.35% examples, 223689 words/s, in_qsize 5, out_qsize 0\n",
      "2024-01-17 20:38:41,609 : - EPOCH 10: training on 540242 raw words (486093 effective words) took 1.9s, 252884 effective words/s\n",
      "2024-01-17 20:38:42,629 : - EPOCH 11 - PROGRESS: at 55.60% examples, 268887 words/s, in_qsize 6, out_qsize 0\n",
      "2024-01-17 20:38:43,404 : - EPOCH 11: training on 540242 raw words (485992 effective words) took 1.8s, 273296 effective words/s\n",
      "2024-01-17 20:38:44,422 : - EPOCH 12 - PROGRESS: at 55.60% examples, 268678 words/s, in_qsize 6, out_qsize 0\n",
      "2024-01-17 20:38:45,177 : - EPOCH 12: training on 540242 raw words (486223 effective words) took 1.8s, 276260 effective words/s\n",
      "2024-01-17 20:38:46,194 : - EPOCH 13 - PROGRESS: at 53.76% examples, 259866 words/s, in_qsize 5, out_qsize 0\n",
      "2024-01-17 20:38:46,979 : - EPOCH 13: training on 540242 raw words (486132 effective words) took 1.8s, 271810 effective words/s\n",
      "2024-01-17 20:38:48,050 : - EPOCH 14 - PROGRESS: at 57.46% examples, 263611 words/s, in_qsize 5, out_qsize 0\n",
      "2024-01-17 20:38:48,770 : - EPOCH 14: training on 540242 raw words (486055 effective words) took 1.8s, 273471 effective words/s\n",
      "2024-01-17 20:38:49,837 : - EPOCH 15 - PROGRESS: at 51.91% examples, 238938 words/s, in_qsize 6, out_qsize 0\n",
      "2024-01-17 20:38:50,641 : - EPOCH 15: training on 540242 raw words (486270 effective words) took 1.9s, 261836 effective words/s\n",
      "2024-01-17 20:38:51,669 : - EPOCH 16 - PROGRESS: at 46.35% examples, 222885 words/s, in_qsize 5, out_qsize 0\n",
      "2024-01-17 20:38:52,671 : - EPOCH 16 - PROGRESS: at 94.38% examples, 228212 words/s, in_qsize 4, out_qsize 0\n",
      "2024-01-17 20:38:52,758 : - EPOCH 16: training on 540242 raw words (486189 effective words) took 2.1s, 231822 effective words/s\n",
      "2024-01-17 20:38:53,784 : - EPOCH 17 - PROGRESS: at 48.21% examples, 231836 words/s, in_qsize 5, out_qsize 0\n",
      "2024-01-17 20:38:54,807 : - EPOCH 17 - PROGRESS: at 96.23% examples, 230205 words/s, in_qsize 3, out_qsize 0\n",
      "2024-01-17 20:38:54,845 : - EPOCH 17: training on 540242 raw words (486148 effective words) took 2.1s, 234886 effective words/s\n",
      "2024-01-17 20:38:55,926 : - EPOCH 18 - PROGRESS: at 51.91% examples, 236095 words/s, in_qsize 6, out_qsize 0\n",
      "2024-01-17 20:38:56,815 : - EPOCH 18: training on 540242 raw words (486200 effective words) took 2.0s, 248535 effective words/s\n",
      "2024-01-17 20:38:57,889 : - EPOCH 19 - PROGRESS: at 57.46% examples, 263113 words/s, in_qsize 5, out_qsize 0\n",
      "2024-01-17 20:38:58,576 : - EPOCH 19: training on 540242 raw words (486094 effective words) took 1.7s, 278193 effective words/s\n",
      "2024-01-17 20:38:59,619 : - EPOCH 20 - PROGRESS: at 57.46% examples, 270744 words/s, in_qsize 5, out_qsize 0\n",
      "2024-01-17 20:39:00,332 : - EPOCH 20: training on 540242 raw words (486223 effective words) took 1.7s, 278998 effective words/s\n",
      "2024-01-17 20:39:01,429 : - EPOCH 21 - PROGRESS: at 51.91% examples, 232159 words/s, in_qsize 5, out_qsize 0\n",
      "2024-01-17 20:39:02,269 : - EPOCH 21: training on 540242 raw words (486164 effective words) took 1.9s, 252670 effective words/s\n",
      "2024-01-17 20:39:03,322 : - EPOCH 22 - PROGRESS: at 53.76% examples, 250362 words/s, in_qsize 6, out_qsize 0\n",
      "2024-01-17 20:39:04,127 : - EPOCH 22: training on 540242 raw words (486177 effective words) took 1.8s, 263149 effective words/s\n",
      "2024-01-17 20:39:05,208 : - EPOCH 23 - PROGRESS: at 57.46% examples, 262372 words/s, in_qsize 5, out_qsize 0\n",
      "2024-01-17 20:39:05,917 : - EPOCH 23: training on 540242 raw words (486180 effective words) took 1.8s, 274342 effective words/s\n",
      "2024-01-17 20:39:06,956 : - EPOCH 24 - PROGRESS: at 57.46% examples, 271745 words/s, in_qsize 5, out_qsize 0\n",
      "2024-01-17 20:39:07,625 : - EPOCH 24: training on 540242 raw words (486219 effective words) took 1.7s, 286841 effective words/s\n",
      "2024-01-17 20:39:08,654 : - EPOCH 25 - PROGRESS: at 57.46% examples, 274455 words/s, in_qsize 6, out_qsize 0\n",
      "2024-01-17 20:39:09,305 : - EPOCH 25: training on 540242 raw words (486155 effective words) took 1.7s, 291673 effective words/s\n",
      "2024-01-17 20:39:10,329 : - EPOCH 26 - PROGRESS: at 57.46% examples, 276122 words/s, in_qsize 5, out_qsize 0\n",
      "2024-01-17 20:39:10,998 : - EPOCH 26: training on 540242 raw words (486118 effective words) took 1.7s, 289516 effective words/s\n",
      "2024-01-17 20:39:12,013 : - EPOCH 27 - PROGRESS: at 57.46% examples, 278490 words/s, in_qsize 6, out_qsize 0\n",
      "2024-01-17 20:39:12,719 : - EPOCH 27: training on 540242 raw words (486107 effective words) took 1.7s, 284621 effective words/s\n",
      "2024-01-17 20:39:13,778 : - EPOCH 28 - PROGRESS: at 57.46% examples, 266871 words/s, in_qsize 6, out_qsize 0\n",
      "2024-01-17 20:39:14,482 : - EPOCH 28: training on 540242 raw words (486224 effective words) took 1.7s, 278108 effective words/s\n",
      "2024-01-17 20:39:15,536 : - EPOCH 29 - PROGRESS: at 57.46% examples, 268750 words/s, in_qsize 5, out_qsize 0\n",
      "2024-01-17 20:39:16,198 : - EPOCH 29: training on 540242 raw words (486072 effective words) took 1.7s, 285857 effective words/s\n",
      "2024-01-17 20:39:16,199 : - Word2Vec lifecycle event {'msg': 'training on 16207260 raw words (14584689 effective words) took 55.4s, 263237 effective words/s', 'datetime': '2024-01-17T20:39:16.199403', 'gensim': '4.3.0', 'python': '3.11.5 | packaged by Anaconda, Inc. | (main, Sep 11 2023, 13:26:23) [MSC v.1916 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.22621-SP0', 'event': 'train'}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(14584689, 16207260)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Criando um modelo Word2Vec - Skip-gram\n",
    "w2v_modelo_sg = Word2Vec(sg=1, # Skip-gram\n",
    "                      window=5, # Considerando 5 palavras antes e depois.\n",
    "                      vector_size=300, # Tamanho do vetor do Word2Vec\n",
    "                      min_count=5, # Considerar apenas palavras que aparecem 5 vezes ou mais. \n",
    "                      alpha=0.03, # Taxa de aprendizado.\n",
    "                      min_alpha=0.007) # Taxa de aprendizado (minima)\n",
    "\n",
    "\n",
    "# Criando o vocabulário do modelo Word2Vec.\n",
    "w2v_modelo_sg.build_vocab(lista_lista_tokens, # Lista de lista de tokens de nossos títulos.\n",
    "                       progress_per=5000) # A cada 5000 títulos, imprime uma mensagem de progresso.\n",
    "\n",
    "# Treinando o modelo Word2Vec.\n",
    "w2v_modelo_sg.train(lista_lista_tokens, # Lista de lista de tokens de nossos títulos.\n",
    "                 total_examples=w2v_modelo_sg.corpus_count, # Número total de exemplos.\n",
    "                 epochs=30) # Número de épocas de treinamento.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Salvando os modelos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-01-17 20:39:16,252 : - storing 12924x300 projection weights into modelo_cbow.txt\n",
      "2024-01-17 20:39:20,841 : - storing 12924x300 projection weights into modelo_skipgram.txt\n"
     ]
    }
   ],
   "source": [
    "w2v_modelo.wv.save_word2vec_format('Dados/modelo_cbow.txt', binary=False)\n",
    "w2v_modelo_sg.wv.save_word2vec_format('Dados/modelo_skipgram.txt', binary=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
